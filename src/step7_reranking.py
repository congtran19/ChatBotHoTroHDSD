"""
Step 7: Re-ranking
S·∫Øp x·∫øp l·∫°i k·∫øt qu·∫£ search ƒë·ªÉ tƒÉng ƒë·ªô ch√≠nh x√°c
"""
import torch
from transformers import AutoModelForSequenceClassification, AutoTokenizer
from typing import List
from step6_vector_database import SearchResult


class Reranker:
    """
    Re-ranker s·ª≠ d·ª•ng transformer model ƒë·ªÉ score l·∫°i query-document pairs.
    
    Input: List[SearchResult] t·ª´ vector search
    Output: List[SearchResult] ƒë∆∞·ª£c s·∫Øp x·∫øp l·∫°i v·ªõi rerank_score m·ªõi
    
    L·ª£i √≠ch:
    - Ch√≠nh x√°c h∆°n bi-encoder (cosine similarity)
    - Xem x√©t interaction gi·ªØa query v√† document
    - Gi·ªØ nguy√™n t·∫•t c·∫£ metadata (page, entities, source, etc.)
    """
    
    def __init__(self, model_path: str = "AITeamVN/Vietnamese_Reranker"):
        """
        Args:
            model_path: HuggingFace model path
                - Default: AITeamVN/Vietnamese_Reranker (Vietnamese-optimized)
                - Alternative: cross-encoder/ms-marco-MiniLM-L-12-v2 (multilingual)
        """
        self.model_path = model_path
        self.tokenizer = AutoTokenizer.from_pretrained(model_path, use_fast=False)
        self.model = AutoModelForSequenceClassification.from_pretrained(model_path)
        self.model.eval()
    def rerank(
        self, 
        query: str, 
        search_results: List[SearchResult], 
        top_k: int = None,
        max_length: int = 512
    ) -> List[SearchResult]:
        """
        Re-rank search results d·ª±a tr√™n query.
        
        Args:
            query: C√¢u h·ªèi c·ªßa user
            search_results: K·∫øt qu·∫£ t·ª´ vector search (SearchResult objects)
            top_k: S·ªë l∆∞·ª£ng k·∫øt qu·∫£ tr·∫£ v·ªÅ (None = tr·∫£ v·ªÅ t·∫•t c·∫£)
            max_length: Max token length cho model
            
        Returns:
            List[SearchResult] ƒë√£ ƒë∆∞·ª£c re-rank, sorted theo rerank_score (cao ‚Üí th·∫•p)
        """
        if not search_results:
            return []
        
        # Prepare query-document pairs
        documents = [result.content for result in search_results]
        pairs = [[query, doc] for doc in documents]
        
        # Score with model
        with torch.no_grad():
            inputs = self.tokenizer(
                pairs, 
                padding=True, 
                truncation=True, 
                return_tensors='pt', 
                max_length=max_length
            )      
            # Get scores
            scores = self.model(**inputs, return_dict=True).logits.view(-1).float()
        
        # Create reranked results (preserve all metadata, update score)
        reranked_results = []
        for result, score in zip(search_results, scores):
            # Create new SearchResult with updated score
            reranked_result = SearchResult(
                content=result.content,
                metadata=result.metadata,  # Keep ALL metadata!
                score=float(score),  # New rerank score
                chunk_id=result.chunk_id,
                knn_score=result.knn_score,  # Keep original kNN score
                bm25_score=result.bm25_score  # Keep original BM25 score
            )
            reranked_results.append(reranked_result)
        
        # Sort by new score (descending)
        reranked_results.sort(key=lambda x: x.score, reverse=True)
        
        # Return top_k if specified
        if top_k:
            return reranked_results[:top_k]
        return reranked_results
    
#test
if __name__ == "__main__":
    print("="*80)
    print("üß™ TEST RERANKER")
    print("="*80)
    
    reranker = Reranker("AITeamVN/Vietnamese_Reranker")
    
    # ============ TEST 2: Reranking with SearchResult (Real Use Case) ============
    print("\n\nüîç TEST 2: Reranking with SearchResult (with metadata)")
    print("-" * 80)
    
    from step6_vector_database import SearchResult, ElasticsearchVectorStore
    from step5_embedding import VietnameseEmbedder
    
    
    query = "L√†m th·∫ø n√†o ƒë·ªÉ ƒë·ªïi m·∫≠t kh·∫©u?"
    query_embedded = VietnameseEmbedder().embed_query(query)
    query_embedded = query_embedded.tolist() if hasattr(query_embedded, 'tolist') else query_embedded
    results = ElasticsearchVectorStore().search_hybrid(query, query_embedded, top_k=5)
    print(f"Query: {query}")
    print(f"Initial results: {len(results)}\n")
    
    print("BEFORE re-ranking (sorted by kNN score):")
    for i, result in enumerate(results, 1):
        print(f"\n[{i}] kNN Score: {result.knn_score:.4f} | Page: {result.metadata['page']}")
        print(f"    Content: {result.content}...")
    
    # Re-rank
    reranked = reranker.rerank(query, results, top_k=3)
    
    print("\n\nAFTER re-ranking:")
    for i, result in enumerate(reranked, 1):
        print(f"\n[{i}] Rerank Score: {result.score:.4f} (kNN: {result.knn_score:.4f}, BM25: {result.bm25_score:.4f})")
        print(f"    Page: {result.metadata['page']} | Source: {result.metadata['source']}")
        print(f"    Section: {result.metadata.get('h2', 'N/A')}")
        print(f"    Chunk ID: {result.chunk_id}")
        print(f"    Content: {result.content}...")
    
    print("\n" + "="*80)
    print("‚úÖ ALL TESTS COMPLETED")
    print("="*80)